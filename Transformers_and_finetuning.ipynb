{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMtIxlJRz3RHD4Gh0yaytr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7d1faee0ae624d6f9912ccde7b98bc3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2088682864a44ec68b9e5e04a121dc9a",
              "IPY_MODEL_f68343ef96d74d62873db406f3f7270c",
              "IPY_MODEL_a655c815406a4084b8c77a15ea601855"
            ],
            "layout": "IPY_MODEL_492d664b3ffb43f5bf6d0f17caf90714"
          }
        },
        "2088682864a44ec68b9e5e04a121dc9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_236d311162ff4ab0835b4cdb5336aa89",
            "placeholder": "​",
            "style": "IPY_MODEL_e3b385d1216f45c79ca3d51684482862",
            "value": "Tokenizing dataset: 100%"
          }
        },
        "f68343ef96d74d62873db406f3f7270c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f776209aaad4ce5b7348e04ab986b48",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_060566d5448c45f1b0e5aa9e36bc5288",
            "value": 2
          }
        },
        "a655c815406a4084b8c77a15ea601855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ce818d649374d7094b38dac9ab17a19",
            "placeholder": "​",
            "style": "IPY_MODEL_bd20a91901be43ffa17a9d4af174b8f4",
            "value": " 2/2 [00:00&lt;00:00, 25.36 examples/s]"
          }
        },
        "492d664b3ffb43f5bf6d0f17caf90714": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "236d311162ff4ab0835b4cdb5336aa89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3b385d1216f45c79ca3d51684482862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f776209aaad4ce5b7348e04ab986b48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "060566d5448c45f1b0e5aa9e36bc5288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ce818d649374d7094b38dac9ab17a19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd20a91901be43ffa17a9d4af174b8f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praneeth-18/Transformers-and-Finetuning-with-LLMs/blob/main/Transformers_and_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NanoGPT Implementation**"
      ],
      "metadata": {
        "id": "RzX4__Kt5WyL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-NwdxrgY5GjT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size, block_size=64, n_embd=128, n_head=4,\n",
        "                 n_layer=4, dropout=0.1, learning_rate=3e-4, max_iters=5000,\n",
        "                 eval_interval=100, batch_size=32, eval_iters=200):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_embd = n_embd\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "        self.eval_interval = eval_interval\n",
        "        self.batch_size = batch_size\n",
        "        self.eval_iters = eval_iters\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, block_size):\n",
        "        chars = sorted(list(set(text)))\n",
        "        self.vocab_size = len(chars)\n",
        "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
        "        self.block_size = block_size\n",
        "        self.data = self.encode(text)\n",
        "\n",
        "    def encode(self, text):\n",
        "        return torch.tensor([self.stoi[c] for c in text], dtype=torch.long)\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ''.join([self.itos[int(i)] for i in tokens])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.head_size = config.n_embd // config.n_head\n",
        "\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.attention_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
        "        k = self.key(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
        "        v = self.value(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attention_dropout(att)\n",
        "\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class SimpleGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.position_embedding = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "def train_model(model, train_loader, val_loader, config, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    for iter in range(config.max_iters):\n",
        "        if iter % config.eval_interval == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            for batch in val_loader:\n",
        "                x, y = [t.to(device) for t in batch]\n",
        "                with torch.no_grad():\n",
        "                    _, loss = model(x, y)\n",
        "                val_loss += loss.item()\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f'Step {iter}: val_loss = {val_loss:.4f}')\n",
        "            model.train()\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x, y = [t.to(device) for t in batch]\n",
        "            _, loss = model(x, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example:**"
      ],
      "metadata": {
        "id": "sDEAFxwE5bXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First paste all the model implementation code above\n",
        "# Then use this simple training script:\n",
        "\n",
        "# Define text directly\n",
        "text = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, \"and what is the use of a book,\" thought Alice \"without pictures or conversations?\"\n",
        "\n",
        "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\"\"\"\n",
        "\n",
        "# Create dataset and model\n",
        "# Setup with reduced training\n",
        "dataset = TextDataset(text, block_size=32)\n",
        "config = GPTConfig(\n",
        "    vocab_size=dataset.vocab_size,\n",
        "    block_size=32,\n",
        "    n_embd=64,\n",
        "    n_layer=4,\n",
        "    max_iters=500,    # Reduced from 2000 to 500\n",
        "    eval_interval=50  # Print every 50 steps instead of 100\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_size = int(0.9 * len(dataset))\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
        "\n",
        "# Train and generate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SimpleGPT(config)\n",
        "model = train_model(model, train_loader, val_loader, config)\n",
        "\n",
        "# Generate text\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=200)[0]\n",
        "print(\"\\nGenerated text:\")\n",
        "print(dataset.decode(generated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWhgPMv25J9E",
        "outputId": "b58adea7-c07b-4914-e7bd-7d5a19d3a9c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: val_loss = 3.7072\n",
            "Step 50: val_loss = 0.4924\n",
            "Step 100: val_loss = 0.2446\n",
            "Step 150: val_loss = 0.2269\n",
            "Step 200: val_loss = 0.2341\n",
            "Step 250: val_loss = 0.2363\n",
            "Step 300: val_loss = 0.2419\n",
            "Step 350: val_loss = 0.2453\n",
            "Step 400: val_loss = 0.2470\n",
            "Step 450: val_loss = 0.2485\n",
            "\n",
            "Generated text:\n",
            "\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasureasure of making a daisy-chain would be worth the trouble of ge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First paste all the model implementation code above\n",
        "# Then use this simple training script:\n",
        "\n",
        "# Define text directly\n",
        "text = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, \"and what is the use of a book,\" thought Alice \"without pictures or conversations?\"\n",
        "\n",
        "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\"\"\"\n",
        "\n",
        "# Create dataset and model\n",
        "# Setup with reduced training\n",
        "dataset = TextDataset(text, block_size=32)\n",
        "config = GPTConfig(\n",
        "    vocab_size=dataset.vocab_size,\n",
        "    block_size=32,\n",
        "    n_embd=64,\n",
        "    n_layer=4,\n",
        "    max_iters=2000,\n",
        "    eval_interval=100\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_size = int(0.9 * len(dataset))\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
        "\n",
        "# Train and generate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SimpleGPT(config)\n",
        "model = train_model(model, train_loader, val_loader, config)\n",
        "\n",
        "# Generate text\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=200)[0]\n",
        "print(\"\\nGenerated text:\")\n",
        "print(dataset.decode(generated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHjOCXki6OZo",
        "outputId": "a91f3865-944d-41af-9abb-586e262eee48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: val_loss = 3.6487\n",
            "Step 100: val_loss = 0.2770\n",
            "Step 200: val_loss = 0.2754\n",
            "Step 300: val_loss = 0.2826\n",
            "Step 400: val_loss = 0.2889\n",
            "Step 500: val_loss = 0.2920\n",
            "Step 600: val_loss = 0.3003\n",
            "Step 700: val_loss = 0.3031\n",
            "Step 800: val_loss = 0.2986\n",
            "Step 900: val_loss = 0.3055\n",
            "Step 1000: val_loss = 0.3072\n",
            "Step 1100: val_loss = 0.3134\n",
            "Step 1200: val_loss = 0.3070\n",
            "Step 1300: val_loss = 0.3088\n",
            "Step 1400: val_loss = 0.3107\n",
            "Step 1500: val_loss = 0.3107\n",
            "Step 1600: val_loss = 0.3167\n",
            "Step 1700: val_loss = 0.3189\n",
            "Step 1800: val_loss = 0.3227\n",
            "Step 1900: val_loss = 0.3271\n",
            "\n",
            "Generated text:\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8SF1hDx9ZP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Case Study: textbooks are all you need**"
      ],
      "metadata": {
        "id": "joFWVMmZH0mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "\n",
        "# Basic setup\n",
        "!pip install transformers datasets torch accelerate\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "import gc\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Initial memory cleared\")\n",
        "\n",
        "# Training data embedded directly\n",
        "training_examples = [\n",
        "    \"\"\"def binary_search(arr, target):\n",
        "    left, right = 0, len(arr) - 1\n",
        "    while left <= right:\n",
        "        mid = (left + right) // 2\n",
        "        if arr[mid] == target:\n",
        "            return mid\n",
        "        elif arr[mid] < target:\n",
        "            left = mid + 1\n",
        "        else:\n",
        "            right = mid - 1\n",
        "    return -1\"\"\",\n",
        "\n",
        "    \"\"\"def quicksort(arr):\n",
        "    if len(arr) <= 1:\n",
        "        return arr\n",
        "    pivot = arr[len(arr) // 2]\n",
        "    left = [x for x in arr if x < pivot]\n",
        "    middle = [x for x in arr if x == pivot]\n",
        "    right = [x for x in arr if x > pivot]\n",
        "    return quicksort(left) + middle + quicksort(right)\"\"\"\n",
        "]\n",
        "\n",
        "# Create dataset with labels\n",
        "dataset = Dataset.from_dict({\n",
        "    'text': training_examples\n",
        "})\n",
        "\n",
        "# Initialize tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Tokenizer loaded\")\n",
        "\n",
        "# Tokenize dataset with labels\n",
        "def tokenize_with_labels(examples):\n",
        "    encodings = tokenizer(\n",
        "        examples['text'],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    encodings['labels'] = encodings['input_ids'].clone()\n",
        "    return encodings\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_with_labels,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Tokenizing dataset\"\n",
        ")\n",
        "print(\"Dataset prepared with labels\")\n",
        "\n",
        "# Load model with FP32\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,  # Changed to FP32\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"Model loaded\")\n",
        "\n",
        "# Training arguments without FP16\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=1,\n",
        "    report_to=\"none\",\n",
        "    learning_rate=1e-5,\n",
        "    fp16=False,  # Disabled FP16\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=0,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_steps=10,\n",
        "    save_strategy=\"no\"\n",
        ")\n",
        "\n",
        "# Custom data collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "print(\"Trainer initialized\")\n",
        "\n",
        "# Train with error handling\n",
        "print(\"Starting training...\")\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    print(\"Training completed!\")\n",
        "    print(f\"Training metrics: {train_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"Training error: {e}\")\n",
        "    raise e\n",
        "\n",
        "# Test generation\n",
        "def generate_code(prompt, max_length=128):\n",
        "    print(f\"\\nGenerating code for prompt: {prompt}\")\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            num_beams=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test with different prompts\n",
        "test_prompts = [\n",
        "    \"def calculate_sum(numbers):\",\n",
        "    \"def is_prime(n):\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting model...\")\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_code(prompt)\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Generated:\\n{generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7d1faee0ae624d6f9912ccde7b98bc3c",
            "2088682864a44ec68b9e5e04a121dc9a",
            "f68343ef96d74d62873db406f3f7270c",
            "a655c815406a4084b8c77a15ea601855",
            "492d664b3ffb43f5bf6d0f17caf90714",
            "236d311162ff4ab0835b4cdb5336aa89",
            "e3b385d1216f45c79ca3d51684482862",
            "0f776209aaad4ce5b7348e04ab986b48",
            "060566d5448c45f1b0e5aa9e36bc5288",
            "2ce818d649374d7094b38dac9ab17a19",
            "bd20a91901be43ffa17a9d4af174b8f4"
          ]
        },
        "id": "uHF7Dw_HH4Wu",
        "outputId": "714a25ac-fe8e-4d66-8927-5c29753a54a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Initial memory cleared\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d1faee0ae624d6f9912ccde7b98bc3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset prepared with labels\n",
            "Loading model...\n",
            "Model loaded\n",
            "Trainer initialized\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.614800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed!\n",
            "Training metrics: TrainOutput(global_step=2, training_loss=2.663395881652832, metrics={'train_runtime': 0.4543, 'train_samples_per_second': 8.804, 'train_steps_per_second': 4.402, 'total_flos': 130648375296.0, 'train_loss': 2.663395881652832, 'epoch': 2.0})\n",
            "\n",
            "Testing model...\n",
            "\n",
            "Generating code for prompt: def calculate_sum(numbers):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: def calculate_sum(numbers):\n",
            "Generated:\n",
            "def calculate_sum(numbers):\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating code for prompt: def is_prime(n):\n",
            "\n",
            "Prompt: def is_prime(n):\n",
            "Generated:\n",
            "def is_prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $prime(n): $\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JozDbvKQMBWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}